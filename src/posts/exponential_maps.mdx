export const frontmatter = {
    title: "Exponential Maps",
    date: "2020-10-25",
    tags: ["math", "robotics"],
    draft: true,
    log: true
};

# Axis-Angle Representation and Exponential Maps

A skew-symmetric matrix, sometimes called an antisymmetric matrix, is a square matrix where $a_{ij}=-a_{ji}$. This means that if the matrix is real, then its diagonal consists only of zeros. For example, the following matrix is skew-symmetric:

$$
\begin{pmatrix}
0 & -1 & 2 \\
1 & 0 & 2 \\
-2 & -2 & 0
\end{pmatrix}
$$

We can use the matrix transpose to define a skew-symmetric matrix.

$$
A^{T} = -A \iff A \text{ is skew-symmetric}
$$

Odd powers of skew-matrices and anti-symmetric, while even powers of skew-matrices are symmetric:

$$
\begin{aligned}
A^{2k}=\underbrace{(A\cdot A) \dots (A \cdot A)}_\text{k pairs} = \underbrace{(-AA^{T}) \dots (-AA^{T})}_\text{k pairs} =\\ (-1)^{k}(AA^{T})^{k}=(-1)^{k}((AA^{T})^{T})^{k} = (-1)^{k}((AA^{T})^{k})^{T} = (A^{2k})^{T} \Rightarrow \\
\text{if A is skew-symmetric, then } A^{2k} \text{ is symmetric}
\end{aligned}
$$

The proof for $A^{2k+1}$ is similar and omitted for bravity.

The powers of a $3\times 3$ skew matrix follow an even more interesting trend. Consider the general form of a $3\times 3$ skew matrix:

$$
A = \begin{pmatrix} 0 & a & b \\ -a & 0 & c \\ -b & -c & 0 \end{pmatrix}
$$

We calculate $A^{2}$. Note that we only need to calculate the upper left triangular matrix, since we know $A^{2}$ is symmetric:

$$
A^{2}=AA = \begin{pmatrix} 0 & a & b \\ -a & 0 & c \\ -b & -c & 0 \end{pmatrix} \begin{pmatrix} 0 & a & b \\ -a & 0 & c \\ -b & -c & 0 \end{pmatrix} = \begin{pmatrix} -a^2-b^2 & -bc & ac \\ -bc & -a^2-c^2 & -ab \\ ac & -ab & -b^2-c^2 \end{pmatrix}
$$

Calculating $A^{3}$ (note that we only need to calculate values above the main diagonal since we know it is skew), we get:

$$
\begin{aligned}
A^{3}=\begin{pmatrix} -a^2-b^2 & -bc & ac \\ -bc & -a^2-c^2 & -ab \\ ac & -ab & -b^2-c^2 \end{pmatrix}\begin{pmatrix} 0 & a & b \\ -a & 0 & c \\ -b & -c & 0 \end{pmatrix} = \\
\begin{pmatrix}
0 & -a^3-ab^2-ac^2 & -a^2b-b^3-bc^2 \\
a^3+ab^2+ac^2 & 0 & -b^2c-a^2c-c^3 \\
a^2b+b^3+bc^2 & b^2c+a^2c+c^3 & 0 \\
\end{pmatrix} = \\
(a^2+b^2+c^2)
\begin{pmatrix}
0 & -a & -b \\
a & 0 & -c \\
b & c & 0
\end{pmatrix} = \\
-(a^2+b^2+c^2)A
\end{aligned}
$$

If $a^2+b^2+c^2 =1$, we have $A^{3}=-A$, which can be generalized to $A^{k+2}=-A^{k}$ for all $k \in \mathbb{N}$, meaning that the power series of such skew matrix looks like this:

$$
I, A, A^2, -A, -A^2, A, A^2, -A, -A^2 \dots
$$

This property will be useful to us later when we discuss the matrix exponential of an axis-angle vector.

An interesting relation between skew-symmetric matrix and cross products of 3 dimensional vectors is that every cross-product can be represented as a skew symmetric matrix:

$$
\begin{aligned}
a \times b = \begin{pmatrix}
a_{y}b_{z}-a_{z}b_{y} \\
a_{z}b_{x}-a_{x}b_{z} \\
a_{x}b_{y}-a_{y}b_{x}
\end{pmatrix} = \\
\begin{pmatrix}
0 \\
a_{z} \\
-a_{y}
\end{pmatrix}
\hat{b_{x}}
+
\begin{pmatrix}
-a_{z} \\ 0 \\ a_{x}
\end{pmatrix}
\hat{b_{y}}
+
\begin{pmatrix}
a_{y} \\ -a_{x} \\ 0
\end{pmatrix}
\hat{b_{z}}
= \\
\begin{pmatrix}
0 & -a_{z} & a_{y} \\
a_{z} & 0 & -a_{x} \\
-a_{y} & a_{x} & 0
\end{pmatrix}
b 
\end{aligned}
$$

Notice how the matrix the vector $b$ is multiplied by is skew-symmetric. We denote this specific form of a skew-symmetric matrix, constructed from the components of a vector $a$, by $[a]_{\times}$, or $S_{a}$:

$$
a \times b = [a]_{\times}b=S_{a}
$$

TODO maybe edit out: Reminder the geometric interpretation of the cross product: its size the area of the parallelogram spanned by the two vectors, and its direction is perpendicular to both vectors.

## Matrix Exponential

The matrix exponential is a generalization of the exponential function to matrices. The matrix exponential of a matrix $A$ is a matrix defined as:

$$
e^{A} = \exp(A) = \sum_{k=0}^{\infty} \frac{A^{k}}{k!}
$$

From this definition and basic properties of matrix multiplication, powers and commution of transpose and powers, we can immediately derive the following properties:
$$
\begin{alignat}{}
e^{A^{T}}=(e^{A})^{T} \\
e^{A}e^{B} = e^{A+B} \\
e^{0} = I
\end{alignat}
$$

We can also define the **matrix logarithm** as the inverse of the matrix exponential:
$$
\log(e^{A}) = A
$$

Now consider a skew-symmetric matrix $A$. We have:
$$
e^{A} (e^{A})^{T} \underset{\text{(1)}}{=}e^{A}e^{A^T}\underset{\text{(2)}}{=}e^{A+A^{T}}\underset{A=[a]_{\times}}{=}e^{0}\underset{\text{(3)}}{=}I
$$

But by definition of the inverse matrix, this means that we have:
$$
(e^{A})^{T} = (e^{A})^{-1}
$$

Which means that, for every skew-symmetric matrix $A$, $e^{A}$ is an orthogonal matrix!

Another important property of the matrix exponential is that the determinant of the matrix exponential of a matrix $A$ is the exponential of the trace of the matrix $A$:

$$
\det(e^{A}) = e^{\text{tr}(A)}
$$

This is a result of [Jacobi's formula](https://en.wikipedia.org/wiki/Jacobi%27s_formula#Corollary) which we state without proof.

Consider a skew-symmetric matrix $A$. Recall that $\text{tr}(A)=0$ since the diagonal of a skew-symmetric matrix consists only of zeros. This means that $\det(e^{A})=e^{0}=1$, which means that $e^{A}$ is special orthogonal, or a rotation.

This means that skew-symmetric matrices in 3D are mapped to 3D rotations via matrix exponentiation. Such relations are called [exponential map](https://en.wikipedia.org/wiki/Exponential_map_(Lie_theory)). In particular, this exponential maps members of the lie algebra $so(3)$, i.e. 3D skew-symmetric matrices to the lie group $SO(3)$, i.e 3D special orthogonal matrices.

## Exponential of an Axis-Angle Vector

Consider an axis-angle representation for a rotation, represented by a unit vector $\hat{u}$ and an angle $\theta$. As we've shown, the skew-symmetric matrix $[\theta \hat{u}]_{\times}$ represents a rotation. Let's attempt to derive it going from first principles.

First, we can take the scalar outside of the skew-symmetric matrix:

$$
[\theta \hat{u}]_{\times} = \theta [\hat{u}]_{\times} = \theta S
$$

Consider the form of $S$:
$$
S = 
\begin{pmatrix}
0 & -u_{z} & u_{y} \\
u_{z} & 0 & -u_{x} \\
-u_{y} & u_{x} & 0
\end{pmatrix}
$$

Since $u$ is a unit vector, its components satisfy $u_{z}^2+u_{y}^2+u_{x}^2=1$. We've already shown that the power series of a $3 \times 3$ skew matrix which satisfies this condition is:

$$
I, S, S^2, -S, -S^2, S, S^2, -S, -S^2 \dots
$$

Recall that the taylor expansions for $\sin\theta$ and $\cos\theta$ are:
$$
\sin \theta = \theta - \frac{\theta^3}{3!} + \frac{\theta^5}{5!} - \frac{\theta^7}{7!} + \dots
$$

$$
\begin{aligned}
\cos \theta = 1 - \frac{\theta^2}{2!} + \frac{\theta^4}{4!}-\frac{\theta^6}{6!}+\dots \Rightarrow \\ 
1 - \cos \theta = \frac{\theta^2}{2!}-\frac{\theta^4}{4!}+\frac{\theta^6}{6!}-\dots
\end{aligned}
$$

Let's calculate the matrix exponential of $\theta S$:

$$
\begin{aligned}
\exp (\theta S) = \sum_{k=0}^{\infty} \frac{(\theta S)^{k}}{k!} = \\
I + \theta S + \frac{\theta^2}{2!}S^2 + \frac{\theta^3}{3!}S^3 + \frac{\theta^4}{4!}S^4 + \dots = \\
I + \theta S + \frac{\theta ^2}{2!}S^2 - \frac{\theta^3}{3!}S - \frac{\theta ^4}{4!}S^{2} + \dots = \\
I + (\theta - \frac{\theta^3}{3!}+ \dots)S +(\frac{\theta^2}{2!}-\frac{\theta^4}{4!}+\dots)S^2 = \\
I + (\sin \theta)S + (1-\cos \theta)S^2
\end{aligned}
$$

This formula is known as [Rodrigues' Rotation Formula](https://en.wikipedia.org/wiki/Rodrigues%27_rotation_formula), and is usually proven with more geometrical arguments. We can also write the matrix exponent more verbosally by adding up all the matrices to form one, rather verbose, matrix. It is rather simple to work out the explicit form of $\exp(\theta S)$, so we will omit it from the post.

Finally, we shall prove that this matrix is indeed a rotation around $\hat{u}$. We've already proved that it is a rotation since it is the exponential of a skew matrix. We shall proceed to prove that $\hat{u}$ is an eigenvector of this matrix with eigenvalue $\lambda=1$:

$$
\begin{aligned}
(I+(\sin \theta) S + (1-\cos\theta)S^2)\hat{u} = \\
I + (\sin \theta)S \hat{u} + (1-\cos \theta)S^2 \hat{u} = \\
\hat{u} +  (\sin\theta) \hat{u} \times \hat{u} + (1-\cos \theta) \hat{u} \times (\hat{u} \times \hat{u}) = \\
\hat{u}+0+0 = \hat{u}
\end{aligned}
$$

Proving that axis of rotation is indeed $\hat{u}$. This also means that the logarithm of a rotation matrix is the skew matrix created by the axis-angle vector $\theta \hat{u}$.

## Signularities of Axis-Angle

The axis-angle representation also suffers from having singularities, much like Euler Angles. First, consider the case where $\theta=0$. In this case, we have:

$$
R(0\hat{u}) = I + 0 \cdot S + (1-1)\cdot S^2 = I
$$

Meaning that the matrix is the identity matrix, making the axis of rotation undefined (as it does not effect how we rotate the shape).

Another signularity occurs when $\phi=\theta$. We can explain this by geometrical reasoning - since the axis vector is directed, then for every unit vector $\hat{u}$ there exists an anti-parallel unit vector $-\hat{u}$. In this case, rotating by $\pi$ along $\hat{u}$ would be the same as rotating by $\pi$ aling $-\hat{u}$, meaning both cases represent the same rotation and we may experience a sign flip-flop when working with rotations near $\theta=\pi$. We can prove this is true using the form of the rotation matrix we 

$$
R(\pi \hat{u}) = I + 0 \cdot S + (1+1)\cdot S^2 = I + 2S^2
$$

Multiplying by some vector $\vec{v}$:

$$
(I+2S^2)\vec{v} = \vec{v}+2(\hat{u} \times(\hat{u} \times \vec{v}))
$$

Now, for $\vec{v}$ to be an eigenvector with eigenvalue 1, we require:

$$
\vec{v}+2(\hat{u} \times(\hat{u} \times \vec{v})) = \vec{v} \iff \hat{u} \times (\hat{u} \times \vec{v}) = 0 \iff \vec{v} = \pm \hat{u}
$$

Thus both $\hat{u}$ and $-\hat{u}$ can act as the axis of rotation when $\theta=\pi$, proving our geometrical argument.

## Derivative of a Rotation Matrix

Consider a rotation matrix $R$. We can express $R$ as the matrix exponential of some skew-symmetric matrix $[a]_{\times}$. The derivative of a matrix exponential parametrized by $t$ is given by $(e^{A(t)})'=A'(t)e^{A(t)}$. We have shown that for every rotation matrix $R(t)$, we have a vector $x$ such that $e^{[x(t)]_{\times}}=R(t)$, which means that:

$$
\frac{d}{dt}R(t)=(\frac{d}{dt}[x(t)]_{\times})R(t)=[\dot{x}]_{\times}R(t)
$$

todo relate to angular velocity

## Angular Velocity - TODO revise

Angular velocity in 3D of some point $r$ moving in velocity $v$ is given by:

$$
\omega = \frac{r \times v}{||r||^{2}}
$$


$$
\begin{aligned}
||u|| = 1 \\
u \dot u = 1 \\
\frac{du}{dt} \cdot u + u \cdot \frac{du}{dt} = 0 \\
\frac{du}{dt} \cdot u = 0 \\
\end{aligned} \Rightarrow 
$$

import { ChatDots } from "react-bootstrap-icons"

